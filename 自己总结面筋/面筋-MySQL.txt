mysql插件式存储引擎，基于表，而非数据库
！！！1InnoDB支持事务，特点行锁设计、支持外键，支持崩溃后的安全恢复，1.2以后支持全文索引！通过多版本并发控制MVCC获得高并发性。
实现了4种隔离级别，默认可重复读级别。next-key locking避免幻读。
数据存储采用聚集方式（每张表按主键顺序存放，如果未显示定义会生成主键）
InnoDB的行锁，只是在WHERE的主键是有效的，非主键的WHERE都会锁全表的。

InnoDB更适合写密集的的表，如果需要事务支持，并且有较高的并发读取频率
(MyISAM的表锁的粒度太大，所以当该表写并发量较高时，要等待的查询就会很多了)，这时选InnoDB是不错的。

InnoDB: 其数据文件本身就是索引文件。相比MyISAM，索引文件和数据文件是分离的，其表数据文件本身就是按B+Tree组织的一个索引结构，
树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。
这被称为“聚簇索引（或聚集索引！）”。而其余的索引都作为辅助索引（非聚集索引！），
辅助索引的data域存储相应记录主键的值而不是地址，这也是和MyISAM不同的地方。
在根据主索引搜索时，直接找到key所在的节点即可取出数据；在根据辅助索引查找时，则需要先取出主键的值，在走一遍主索引。 
因此，在设计表的时候，不建议使用过长的字段作为主键，因为辅助索引的叶子节点要存储，过长浪费空间
也不建议使用非单调的字段作为主键，这样会造成主索引频繁分裂。

！！！2MyISAM不支持事务、行锁设计，支持全文索引、表锁。只缓存索引文件，不缓存数据文件。
MyISAM更适合读密集的表
如果你的数据量很大（MyISAM支持压缩特性可以减少磁盘的空间占用），而且不需要支持事务时，MyISAM是最好的选择。

MyISAM: B+Tree叶节点的data域存放的是数据记录的地址。在索引检索的时候，首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，
则取出其 data 域的值，然后以 data 域的值为地址读取相应的数据记录。这被称为“非聚簇索引”！

！！！3Memory把数据放到内存，数据库重启或者崩溃数据消失！使用哈希索引，而非B+树。
只支持表锁，并发性能差。

连接mysql本质上是一个连接进程和mysql实例进程通信的过程。

--------------------------------------------------------------------------------

InnoDB
基于磁盘的存储，将记录按页的方式管理。通过缓冲池（一块内存区域，包含数据页、缓冲页等）弥补CPU和磁盘间的速度差异。
读取操作首先判断是否在缓冲池，在则命中。否则从磁盘读取。
修改操作首先修改缓冲池，LRU列表的页被修改后，与磁盘数据不一致，成为脏页，加入Flush列表。

重做日志缓冲默认每秒刷新到重做日志文件，重做日志缓冲的大小不用很大，保证能容纳每秒的事务量即可。
Write Ahead Log策略（！！！事务的崩溃恢复）：当事务提交时，先写重做日志，再修改页。
当发生宕机，通过重做日志恢复数据，即ACID中Duration的要求。
为什么再大的事务提交时间都是很短的？事务提交之前先写重做日志，保证不会丢失即提交。

以一定频率（checkpoint）由后台线程刷新回磁盘（如果一有脏页出现就刷新磁盘，性能低下）。
checkpoint技术：关键是什么时候触发。
Sharp checkpoint：数据库关闭之前，所有脏页刷新回磁盘。
Fuzzy checkpoint：缓冲区不够、重做日志不可用，刷新脏页回磁盘

在进程运行过程中，若其所要访问的页面不在内存而需把它们调入内存，但内存已无空闲空间时，为了保证该进程能正常运行，
系统必须从内存中调出一页程序或数据送磁盘的对换区中。但应将哪个页面调出，须根据一定的算法来确定。
通常，把选择换出页面的算法称为页面置换算法(Page-Replacement Algorithms)。 置换算法的好坏， 将直接影响到系统的性能。
一个好的页面置换算法，应具有较低的页面更换频率。从理论上讲，应将那些以后不再会访问的页面换出，
或把那些在较长时间内不会再访问的页面调出。存在着许多种置换算法，它们都试图更接近于理论上的目标。

内存管理的页面置换算法、缓存淘汰算法
！！！1LRU（Least Recently Used）最久未使用置换算法
（记录一个时间字段每次比较时间不够高效，且时间不是必须记录的！！！
最好利用“双向链表（LinkedList）加HashMap”实现，查找和修改都时O（1））
利用局部性原理，根据一个作业在执行过程中过去的页面访问历史来推测未来的行为。
它认为过去一段时间里不曾被访问过的页面，在最近的将来可能也不会再被访问。
所以，这种算法的实质是：当需要淘汰一个页面时，总是选择在最近一段时间内最久不用的页面予以淘汰。
！！！2OPT最佳置换算法
理想情况，不可能实现的。该算法的基本思想是：发生缺页时，有些页面在内存中，其中有一页将很快被访问（也包含紧接着的下一条指令的那页），
而其他页面则可能要到10、100或者1000条指令后才会被访问，每个页面都可以用在该页面首次被访问前所要执行的指令数进行标记。
最佳页面置换算法只是简单地规定：标记最大的页应该被置换。这个算法唯一的一个问题就是它无法实现。
当缺页发生时，操作系统无法知道各个页面下一次是在什么时候被访问。
！！！3FIFO先进先出置换算法
最简单的页面置换算法是先入先出（FIFO）法。总是选择在主存中停留时间最长（即最老）的一页置换，即先进入内存的页，先退出内存。
理由是：最早调入内存的页，其不再被使用的可能性比刚调入内存的可能性大。建立一个FIFO队列，收容所有在内存中的页。
被置换页面总是在队列头上进行。当一个页面被放入内存时，就把它插在队尾上。
这种算法只是在按线性顺序访问地址空间时才是理想的，否则效率不高。因为那些常被访问的页，往往在主存中也停留得最久，
结果它们因变“老”而不得不被置换出去。FIFO的另一个缺点是，它有一种异常现象，即在增加存储块的情况下，反而使缺页中断率增加了。
！！！4LFU（least frequently used）最少使用置换算法
在采用最少使用置换算法时，应为在内存中的每个页面设置一个"！！！移位寄存器"，用来记录该页面被访问的频率。
该置换算法选择在之前时期使用最少的页面作为淘汰页。由于存储器具有较高的访问速度，例如100 ns，在1 ms时间内可能对某页面连续访问成千上万次，
因此，通常不能直接利用计数器来记录某页被访问的次数，而是采用移位寄存器方式。每次访问某页时，便将该移位寄存器的最高位置1，
再每隔一定时间(例如100 ns)右移一次。这样，在最近一段时间使用最少的页面将是∑Ri最小的页。
LFU置换算法的页面访问图与LRU置换算法的访问图完全相同；或者说，利用这样一套硬件既可实现LRU算法，又可实现LFU算法。
应该指出，LFU算法并不能真正反映出页面的使用情况，因为在每一时间间隔内，只是用寄存器的一位来记录页的使用情况，
因此，访问一次和访问10 000次是等效的。


InnoDB特性：
性能提升！！！插入缓冲insert buffer（需要满足辅助索引不是唯一的，因为判断唯一性需要查找索引页，会出现离散读，导致插入缓冲失去意义）
一般硬盘读取都是指定位置之后，一下子读取这附近挺大的一块数据放到缓存里的，如果是顺序读取的话，可能第二次就直接命中缓存从缓存里拿，
速度快得多。随机的话每次都要重新寻道查找（寻道是机械耗时）。
InnoDB中数据记录的插入是按照主键递增进行插入的(聚集索引)。主键一般是顺序的，不需要磁盘随机读取，如 a int auto_increment   primary key(a)
插入null时，a列自动增长，“同时页中的记录按顺序存放”，不需要随机读取（一般不需要换页寻道），速度快。
（但主键为UUID这种，插入非连续，就会存在多个页，耗时。）
当存在辅助索引如key(b)，非聚集索引，进行插入操作时，数据页的存放还是按主键顺序。但对于非聚集索引页，叶子节点的插入就不是顺序的，
需要离散地访问非聚集索引页，随机读取导致性能下降。
这些都是由B+树的特性决定的。
插入缓冲不是直接插入非聚集索引页，而是判断是否在缓冲池中，是则直接插入。如不在，先放入插入缓冲区中，再以一定的频率合并，
可以累积一个非聚集索引页的多个插入操作再插入，减少换页寻道的次数。

1：为什么会有insert buffer,insert buffer能帮我们解决什么问题？
举个现实中的例子来做说明，我们去图书馆还书，对应图书馆来说，他是做了insert(增加)操作，管理员在1小时内接受了100本书，
这时候他有2种做法把还回来的书归位到书架上
1）每还回来一本书，根据这本书的编码（书柜区-排-号）把书送回架上
2）暂时不做归位操作，先放到柜面上，等不忙的时候，再把这些书按照书柜区-排-号先排好，然后一次性归位
用方法1，管理员需要进出（IO）藏书区100次，不停的登高爬低完成图书归位操作，累死累活，效率很差。
用方法2，管理员只需要进出（IO）藏书区1次，对同一个位置的书，不管多少，都只要爬一次楼梯，大大减轻了管理员的工作量。
所以图书馆都是按照方法2来做还书动作的。但是你要说，我的图书馆就20本书，1个0.5米的架子，方法2和1管理起来都很方便，
这种情况不在我们讨论的范围。当数据量非常小的时候，就不存在效率问题了。
关系数据库在处理插入操作的时候，处理的方法和上面类似，每一次插入都相当于还一本书，它也需要一个柜台来保存插入的数据，
然后分类归档，在不忙的时候做批量的归位。这个柜台就是insert buffer.


数据页的可靠性！！！两次写、（dubble write）
部分写失效：存储引擎正在写某个页到表中，写了一部分数据库宕机了，引起数据丢失。
为什么重做日志记录不可行？重做是对页的物理操作，如果页本身损坏，没有意义重做。
即需要一个页的副本，在写失效时，先还原页再重做。
dubble write分为内存中的缓冲区和磁盘上连续的页。在脏页刷新时，先复制到内存的dubble write缓冲区，再写入dubble write磁盘，
由于页连续，开销不大。
最后将页写入磁盘，如果中途崩溃，用dubble write磁盘的副本恢复，在应用日志重做。


自适应哈希索引、
哈希的时间复杂度O（1），一次查找定位。B+树查找次数取决于树的高度，高度h=logdn，d为每层宽度，n为总数，一般3、4层。
自适应：观测热点，为B+树的某些页建立哈希索引

NIO和BIO区别？NIO怎么写？阻塞和非阻塞，同步IO和异步IO的区别？
！！！异步IO（AIO）、
SIO同步IO，每进行一次IO操作，需要等这次操作结束才能做接下来的操作。
AIO异步IO，用户可以再发出一个IO请求后立即再发出另外一个，当全部IO请求发送完毕，等待IO操作的完成。
异步IO的优势是可以IO merge，如页678同步IO需要三次，异步IO判断连续，底层只会发一个IO请求。
对于异步，通知调用者的方式，具体如下：
状态
即监听被调用者的状态（轮询），调用者需要每隔一定时间检查一次，效率会很低。
回调
与通知类似，当被调用者执行完成后，会调用调用者提供的回调函数，无需消耗太多性能。

！！！同步异步阻塞非阻塞
首先用户（应用层）发起的的IO请求，是发给系统内核的。注意这里是线程级别的。
同步异步指的是是否需要等待IO结果，才能继续执行其他操作。
阻塞非阻塞指的是等内核返回IO资源就绪的阶段（同步等异步等）是否挂起，关注的是程序在等待调用结果时的状态.
同步/异步关注的是消息通知的机制，而阻塞/非阻塞关注的是程序（线程）等待消息通知时的状态。
两个阶段在时间上有先后顺序，可以组合。

1 同步阻塞形式
效率是最低的，
拿上面的例子来说，就是你专心等待下载完成，什么别的事都不做。
实际程序中：就是未对fd 设置O_NONBLOCK标志位的read/write 操作；
2 异步阻塞形式
异步操作是可以被阻塞住的，！！！只不过它不是在处理消息时阻塞，而是在等待消息通知时被阻塞。
比如select 函数，假如传入的最后一个timeout参数为NULL，那么如果所关注的事件没有一个被触发，程序就会一直阻塞在这个select 调用处。
3 同步非阻塞形式
实际上是效率低下的，
想象一下你一边干别的事情一边还需要抬头看下载完成没有，如果把干别的事情和观察下载完成情况的位置看成是程序的两个操作的话，
这个程序需要在这两种不同的行为之间来回的切换，效率可想而知是低下的。
很多人会写阻塞的read/write 操作，但是别忘了可以对fd设置O_NONBLOCK 标志位，这样就可以将同步操作变成非阻塞的了。
4 异步非阻塞形式
效率更高，
因为等待下载完成是你(等待者)的事情，而通知你则是电脑(消息触发机制)的事情，程序没有在两种不同的操作中来回切换。
api和agent如果理解为用户和内核，调用模型理解为IO，则使用了同步非阻塞，异步非阻塞。

刷新邻接页
刷新一个脏页时，检测所在区所有的页，如果是脏页，
通过AIO把多个IO写入合并成一个操作，一起刷新。

----------------------------------------------------------------------------------
1二进制日志会记录对mysql数据库执行更改的所有日志记录，包括InnoDB等其他存储引擎的日志。
！！！可以用于恢复 或者 复制（到远程slave数据库）

2重做日志只记录InnoDB存储引擎有关的事务日志，保证事务的持久性

3回滚日志（undo log）：保存了事务发生之前的数据的一个版本，可以用于回滚，
同时可以提供多版本并发控制下的读（MVCC），也即非锁定读
什么时候产生：事务开始之前，将当前是的版本生成undo log，undo 也会产生 redo 来保证undo log的可靠性
什么时候释放：当事务提交之后，undo log并不能立马被删除，而是放入待清理的链表，
由purge线程判断是否有其他事务在使用undo段中表的上一个事务之前的版本信息，决定是否可以清理undo log的日志空间。

4错误日志：数据库出现问题或者对数据库层面优化。

5慢查询日志：可以对SQL语句层面优化。设置一个时间阈值，超过这个阈值的SQL语句都记录到慢查询日志。
-------------------------------------------------------------------------------------------
索引组织表：表的存储依据主键的顺序，数据段即B+树的叶子节点
主键如果没有显示定义primary key:
1首先寻找非空的唯一索引（unique not null） 设置为主键
2如果没有innodb自动创建一个6字节的自增主键

--------------------------------------------------------------------------------------------
外键(FOREIGN KEY)：处理表之间关系问题，表与表的记录之间存在着三种关系：一对多、多对多、一对一的关系。
一个表中的 FOREIGN KEY 指向另一个表中的 UNIQUE KEY(唯一约束的键)。
约束1：在创建表时，先建被关联的表dep，才能建关联表emp
约束2：在插入记录时，必须先插被关联的表dep，才能插关联表emp
约束3：更新与删除都需要考虑到关联与被关联的关系。

多对一关系
举例：雇员表：emp表   部门：dep表
多个员工是否可以属于一个部门？可以
多个部门是否可以包含同一个员工？不可以
此时就可以用到外键了，在emp表中新增一个dep_id字段，该字段指向dep表的id字段

一对一关系
左表的一条记录唯一对应右表的一条记录，反之也一样。
左表新增一个字段指向右表的一个字段，！！！注意此时该FOREIGN KEY字段一定要是唯一的

两张表记录之间是一个双向的多对一关系，称之为多对多关系
如何实现？建立第三张表，该表中有一个字段foreign key左表的id，还有一个字段是foreign key右表的id
如author表 book表 author-book对应表

--------------------------------------------------------------------------------------------
触发器trigger：在插入、删除、更新操作(3)前、后(2) 自动调用的 sql命令或者存储过程
因此一个表最多3*2=6个触发器
做一些逻辑上的限制，比如余额不能为负值

视图：是select语句组成的一个虚表，没有实际的物理存储。
从内部看由一张或者多张表的数据组成，从外部看是一张表，即也不需要关注基表的结构。
用作抽象和作为安全层。
--------------------------------------------------------------------------------------------
BTREE
B+索引：balance+  多路平衡查找树
B+树：在B-树（B-Tree就是我们常说的B树，一定不要读成B减树，否则就很丢人了。）基础上，
所有关键字（数据）在叶子结点出现，非叶子结点作为叶子结点的索引；B+树总是到叶子结点才命中；
为叶子结点增加链表指针，所以从左边的叶子节点开始遍历，可得到所有键值
B*树：在B+树基础上，为非叶子结点也增加链表指针，将结点的最低利用率从1/2提高到2/3；

B+树索引在DB中有一个特点就是【高扇出性】，一般在DB中B+树的高度在2-3层左右。
也就意味着只需要2-3次的IO操作即可。而现在的磁盘每秒差不多在100次IO左右，2-3次意味着查询时间只需0.02-0.03秒。
！！！为什么数据库选B+tree而不是二叉树作为索引结构？？？
！！！为什么说B+树比B树更适合实际应用中操作系统的文件索引和数据库索引？？？
1.磁盘读取依靠的是机械运动，寻道十分耗时，优化采用"局部预读"：当访问一个地址数据的时候，与其相邻的数据很快也会被访问到。
每次磁盘IO读取的数据我们称之为一页（page）。"一页"的大小与操作系统有关，一般为4k或者8k。
这也就意味着读取一页内数据的时候，实际上发生了一次磁盘IO。
2.数据库索引是存储在磁盘上，索引的大小跟着表中的数据量增长，达到几个G甚至更多。
当我们利用索引进行查询的时候，不可能把索引全部加载到内存中，只能逐一加载每个磁盘页，这里的磁盘页就包含索引树的节点。
“高度h=logdn”（d为每层宽度，n为总数一定）“在每页能容纳的情况下增大d”，高度减小，减少了相对IO的次数。
相对B-树，B+树把数据放在叶子节点，节点占的空间减少，则一页能容纳更多节点相当于d增大，减少了相对IO的次数。
3.相对二叉树的查询效率更加稳定 
由于非终结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。
所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字“查询的路径长度相同”，导致每一个数据的查询效率相当。


HASH
哈希索引：（在innodb使用到了自适应的哈希索引）
为什么大部分场景选择B+树而非哈希索引？？？
1虽然等值查询，哈希索引的时间复杂度O（1）。但经常查询多条数据，“范围”查询，
B+树数据的有序性，叶子节点又有双向链表相连，他的查询效率会比Hash快的多。
2哈希之后变得不连续，没办法利用索引完成“排序”
3在有大量重复键值情况下，产生“哈希碰撞”，哈希索引的效率低。

首先将关键字转换为数值：
hashCode()底层实现，hashcode默认为key在内存中的地址,得到hash值
再用常见的哈希函数：
1H（key）=a*key+b,a、b常数的线性函数
2除法求余
3平方取中

冲突的解决：
1拉链法（数组加链表）
2线性探测法（再散列）

全文索引：full text search
将整段文字中的任意信息查询出来，使用B+树有局限性比如like “%aaa%” 失效。
采用倒排索引技术，full inverted index，文档用列标号，记录下每个单词可能出现在文档每列的具体位置
FULLTEXT KEY
MATCH(CLO1,COL2...) AGAINST(EXPR)
--------------------------------------------------------------------------------------------
为什么只能一张表只能有一个聚集索引（主键）？
数据和主键的顺序一致，但实际的数据只能按照一颗B+树排序

！！！聚集索引的主键顺序对应实际数据怎么实现的，是物理顺序存储的吗？
物理顺序存储带来的维护成本高，实际是按照逻辑顺序。
1所有的页通过双向链表连接，页按照主键顺序排序
2页内的记录也通过双向链表维护。

！！！主键的排序查找和范围查找用聚集索引为什么快？
排序查找：通过双向链表方便找到页，也方便找到周围的记录
范围查找：找到某个叶子节点，通过它的上层中间节点就可以得到页的范围，直接读取数据页。

聚集索引表记录的排列顺序与索引的排列顺序一致
优点是"查询速度快"，只用查询一次；并且适合"范围查询"，因为一旦具有第一个索引值的纪录被找到，具有连续索引值的记录也一定物理的紧跟其后。
缺点是"不适合频繁更新的列",对表进行修改速度较慢，这是为了保持表中的记录的物理顺序与索引的顺序一致，
而把记录插入到数据页的相应位置，必须在数据页中进行数据重排， 降低了执行速度。

非聚集索引：查询两次,不适合范围查询，适合频繁更新的列。


二分查找法（前提有序）--搜索二叉树（无重复元素）--平衡二叉树（任意节点左右子树高度差不大于1）--不严格的平衡二叉树：红黑树（5个条件；左右旋）
红黑树是一种含有红黑结点并能自平衡的二叉查找树。它必须满足下面性质：
性质1：每个节点要么是黑色，要么是红色。
性质2：根节点是黑色。
性质3：每个叶子节点（NIL）是黑色。
性质4：每个红色结点的两个子结点一定都是黑色。
性质5：任意一结点到每个叶子结点的路径都包含数量相同的黑结点。
红黑树并不是一个完美平衡二叉查找树，但任意一个结点到到每个叶子结点的路径都包含数量相同的黑结点(性质5)。
所以我们叫红黑树这种平衡为黑色完美平衡。


索引创建删除的语法：哪张表哪个字段（过长字段可以取一部分建立索引） 建立/删除 什么类型的索引
ALTER TABLE tbname 
ADD TYPE INDEX indexName（colName）

ALTER TABLE tbname 
DROP INDEX indexName
或者
CREATE TYPE INDEX indexName
ON tbName(cloName)

DROP INDEX indexName
ON tbName
其中TYPE包括： PRIMARY KEY主键、UNIQUE唯一索引、INDEX普通索引、FULLTEXT全文索引


什么时候需要加索引？？？
1大量使用查询（如where条件中 及 order by涉及的列上建立索引 ）的列
2记录行数多（不等于单行数据量大），一般超过2000行；
不能是定义为text, image和bit数据类型的列。因为，这些列的数据量太大
3出现“重复率”小的列，如性别男女这种字段就不用加索引。（Cardinality基数表示索引中不重复数量的预估值，Cardinality/n_rows_in_table
要尽可能接近1，如果很小表明重复率高，没必要加索引）
4稳定不频繁变化的字段

！建立时尽量使用短索引，不仅可以提高查询速度而且可以节省磁盘空间和I/O操作。
例如，如果有一个CHAR(255)的 列，如果在前10 个或20 个字符内，多数值是惟一的，那么就不要对整个列进行索引。
！复合索引中只要有一列含有NULL值，那么这一列对于此复合索引就是无效的。所以我们在数据库设计时不要让字段的默认值为NULL。

联合索引的键值大于等于2，即建立在两个及其以上的列上（适用多条件查询）
以联合索引(a,b,c)为例，建立这样的索引相当于建立了索引a、ab、abc三个索引，mysql查询每次只能使用一个索引。
a通常放最常用的，筛选数据最多的字段。因为只有左侧先走了索引，之后的字段才有可能走索引，mysql会首先匹配a，然后再b，c。
！！！索引失效情况？？？
1.最左前缀原理：如果用(b,c)这样的数据来检索的话，就会找不到a使得索引失效。
；如果使用(a,c)这样的数据来检索的话，就会先找到所有a的值然后匹配c，此时联合索引是失效的。
但是，对于联合索引中的字段出现位置，比如查询a,b,还是b,a，并没有严格的要求！！！
2.如果where条件中是OR关系,除非OR的每个条件都加上索引，否则索引失效。
3.like “%aaa%” （like的左模糊）不会使用索引而like “aaa%”可以使用索引。
4.存储引擎不能使用索引中范围条件右边的列。若中间索引列用到了范围（>、<、like等），范围之后全失效。
5.is null、is not null 不走索引，因为索引不能存储全为null的值，只能全表扫描。
6.在索引列上操作（加函数、做计算、（显式隐式）类型转换），导致索引失效
如：列类型是字符串，那一定要在条件中将数据使用引号引用起来,否则发生隐式转换，不会使用索引
7.mysql中使用了“！=”，“<>”,“not in”，“not exist”会使索引失效，但对于“>”，“<”，“>=”，“<=”的使用，
（如果优化器估计使用全表扫描要比使用索引快,则不使用索引。
所要查询的记录数大时，可能通过索引耗费的IO次数（页数约等于所要查询的记录数*h） 大于 全表扫描的IO次数（所有行数所占页数）。
考虑极限情况，查询所有记录，与其走索引再取数据，不如直接走全表扫描，
也解释了重复率高时，查询一个类型所占比例高，没必要建立索引）
https://blog.csdn.net/ystyaoshengting/article/details/43154157
由于索引扫描后要利用索引中的指针去逐一访问记录，假设每个记录都使用索引访问，则读取磁盘的次数是查询包含的记录数T，
而如果表扫描则读取磁盘的次数是存储记录的块数B，如果T>B 的话索引就没有优势了。
对于大多数数据库来说，这个比例是10%，即先对结果数量估算，如果小于这个比例用索引，大于的话即直接表扫描。


！！！
1.单列索引无法储null值，复合索引无法储全为null的值。查询采用is null条件时，不能利用到索引，只能全表扫描。
注意无法存储null值是针对索引无法比较，但是如果允许（唯一索引、普通索引都可为空），还是会存进表里！
2.为什么索引列无法存储Null值？
索引是有序的，将索引列值进行建树，涉及到比较操作，NULL值进入索引时“无法比较”，无法确定其应该放在哪里。
3.如何把空值存入索引？
其一，把NULL值转为一个特定的值，在WHERE中检索时，用该特定值查找。
其二，建立一个复合索引。例如　
create index ind_a on table(col1,1);  通过在复合索引中指定一个非空常量值，而使构成索引的列的组合中，不可能出现全空值。　


覆盖索引是什么？
是非聚集组合索引的一种形式，
就是select的数据列只用从非聚集索引中就能够取得，不需要查询聚集索引中的记录也不必从数据表中读取，
换句话说查询列要被所使用的非聚集索引覆盖。
为什么尽量使用覆盖索引，减少select *？
select col1 from table; 如果col1是一个辅助索引，那么Mysql只需要查询这个辅助索引就够了，
而select * from table除了要查询辅助索引以外，还要再查一次聚集索引，这就就造成了额外的性能开销。


ORDER BY（group by 类似） 通常会有两种实现方法：
1一个是利用有序索引自动实现，也就是说利用有序索引的有序性就不再另做排序操作了（索引在叶子节点本身有序）。
当order by中的字段（或者联合索引的一部分）出现在where条件中时，才会利用索引而不排序
2另一个是把结果选好之后再排序。


---------------------------------------数据库调优(SQL层面、mysql配置层面、数据库表层面)-----------------------------------------
！！！SQL层面（排查执行慢的sql）
1慢查询日志定位查询慢的语句（设定阈值）、没有索引是否需要创建索引、
2explain分析索引是否需要创建、是否失效、什么情况下失效
3尽量将多条SQL语句压缩到一句SQL中，每次执行SQL的时候都要建立网络连接、进行权限校验、进行SQL语句的查询优化、发送执行结果，这个过程 
是非常耗时的
4即使检索结果中不会有重复的记录，如果使用union这两个结果集，同样会尝试进行合并去重，然后在输出最终结果前进行排序，
因此如果可以判断检索结果中不会有重复的记录时候，应该用union all，这样效率就会因此得到提高。
5尽可能用varchar/nvarchar 代替 char/nchar，因为变长字段可以节省存储空间，不要以为 NULL 不需要空间，
比如：char(100) 型，在字段建立时，空间就固定了， 不管是否插入值（NULL也包含在内），都是占用 100个字符的空间的，
如果是varchar这样的变长字段， null 不占用空间。
6程序中通常是根据用户的输入来动态执行SQL，这时应尽量使用参数化SQL进行预编译,这样不仅可以避免SQL注入漏洞 
攻击，最重要数据库第一次执行的时候DBMS会为这个SQL语句进行查询优化并且执行预编译，
这样以后再执行这个SQL的时候就直接使用预编译的结果，这样可以大大提高执行的速度。
7如果不用查找全部字段，避免 select *

！！！mysql配置层面（拒绝默认配置，默认情况下，MySQL 是针对小规模的发布、安装进行调优的，而并非真正的生产环境规模）
innodb_buffer_pool_size
如果您只运行 InnoDB 存储引擎，那么您通常可以分配 80％ 左右的内存给该缓冲池。
要确保其设置既不要过大，也不要频繁引起交换（swapping），因为这些绝对会降低您的数据库性能。
而如果您要运行非常复杂的查询或者您有大量的并发数据库连接，亦或您有非常大的数据表的情况，
那么就可能需要将此值下调一个等级，以便为其他的调用分配更多的内存。
innodb_log_file_size
如果分配了较大的重做空间，那么对于写入密集型的工作负载来说性能会越好。
但是如果您的系统遭受到断电或其他问题导致崩溃的时候，那么其恢复时间则会越长。
max_connections
一个健康的系统，它有着足够数量的可用额外连接。
通常，您需要确保在应用程序所使用到的最大连接数和可用的最大连接数之间至少有 30％ 的差额。

！！！数据库表层面
当MySQL单表记录数过大时，数据库的CRUD性能会明显下降
1限定数据的范围： 务必禁止不带任何限制数据范围条件的查询语句。如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内。

2读/写分离： 经典的数据库拆分方案，主库负责写，从库负责读；

3缓存： 使用MySQL的缓存，另外对重量级、更新少的数据可以考虑使用"应用级别的缓存"；

4垂直分区：
根据数据库里面数据表的相关性进行拆分。 例如，用户表中既有用户的登录信息又有用户的基本信息，可以将用户表拆分成两个单独的表，甚至放到单独的库做分库。
简单来说垂直拆分是指数据表列的拆分，把一张列比较多的表拆分为多张表
垂直拆分的优点：
可以使得行数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护。
垂直拆分的缺点：
主键会出现冗余，需要管理冗余列，并会引起Join操作，可以通过在应用层进行Join来解决。此外，垂直分区会让事务变得更加复杂；

5水平分区：
表的行数超过200万行时，就会变慢，这时可以把一张的表的数据拆成多张表来存放。
水平拆分是指数据表行的拆分，保持数据表结构不变，通过某种策略（按照范围、取模运算、hash运算进行数据切割）存储数据分片。
这样每一片数据分散到不同的表或者库中，达到了分布式的目的。 
需要注意的一点是:分表仅仅是解决了单一表数据过大的问题，
但由于表的数据还是在同一台机器上，其实对于提升MySQL并发能力没有什么意义，所以 水品拆分最好分库 。
水平拆分的优点：
水平拆分能够 支持非常大的数据量存储，应用端改造也少
水平拆分的缺点：
但分片事务难以解决 ，跨界点Join性能较差，逻辑复杂。
-----------------------------------------------------------------------------------

union all union 合并结果集，SELECT语句必须满足对应列 相同数量，相似的数据类型。
[SELECT 语句 1]
UNION/UNION ALL
[SELECT 语句 2]
对重复结果的处理：UNION在进行表链接后会筛选掉重复的记录，Union All不会去除重复记录。
对排序的处理：Union将会按照字段的顺序进行排序；UNION ALL只是简单的将两个结果合并后就返回。
从效率上说，UNION ALL 要比UNION快很多，所以，如果可以确认合并的两个结果集中不包含重复数据且不需要排序时的话，那么就使用UNION ALL。

count（1）count（0） count（*） count(col)
只需要找到属于表的数据块块头，然后计算一下行数就行了，而不用去读取里面数据列的数据。
1.count(0)=count(1)=count(*)
count(指定的有效值)--执行计划都会转化为count(*)，COUNT(*) 函数返回表中的记录数(包括 NULL 值和重复项)
2.COUNT(column_name) 函数返回指定列的值的数目（NULL 不计入）
COUNT(DISTINCT column_name) 函数返回指定列的不同值的数目（NULL 不计入）
如果是对特定的列做count的话建立这个列的非聚集索引能对count有很大的帮助。

当只关心数据表有多少记录行而不需要知道具体的字段值时，类似“select 1 from tblName”减少系统开销，提高运行效率，它通常用于子查询。
因为这样子写的SQL语句，数据库引擎就不会去检索数据表里一条条具体的记录和每条记录里一个个具体的字段值并将它们放到内存里，
而是根据查询到有多少行存在就输出多少个“1”，每个“1”代表有1行记录，同时选用数字1还因为它所占用的内存空间最小，当然用数字0也一样。

in  和  exist（存在与否返回true或者false，可以和“null”对应起来）
有个很重要的区别是，如果在“子查询”的结果里返回了NULL，IN子句会执行失败，因为NULL和任何值都不相等。
当你想对两个表（“外表内表--驱动表”）进行差集操作的时候，可以配合子查询，使用EXISTS 或 IN 。
1.select * from A where id in(select id from B)
in在查询的时候，首先查询子查询的表，然后将内表和外表做一个笛卡尔积，然后按照条件进行筛选。所以相对内表比较小的时候，in的速度较快。
2.select a.* from A a where exists(select 1 from B b where a.id=b.id)
遍历循环外表，然后看外表中的记录有没有和内表的数据一样的，指定一个子查询，检测行的存在。匹配上就将结果放入结果集中。
我们区分in和exists主要是驱动顺序的不同(这是性能变化的关键)，以驱动表的快速返回为目标，就会考虑到索引及结果集的关系了。
如果是exists，那么以外层表为驱动表，先被访问。如果是IN，那么先执行子查询，
如果子查询得出的结果集记录较少，主查询中的表较大且又有索引时应该用in, 
反之如果外层的主查询记录较少，子查询中的表大，又有索引时使用exists。
如果查询语句使用了not in 那么内外表都进行全表扫描，没有用到索引；而not extsts 的子查询依然能用到表上的索引。
所以无论那个表大，用not exists都比not in要快。

The in is best used where you have a static list to pass:
 select * from [table]
 where [field] in (1, 2, 3)
When you have a table in an in statement it makes more sense to use a join, but mostly it shouldn't matter. 
The query optimiser should return the same plan either way，a nested join plan。


between and包含边界（等价>= and <=）  和 in  和 or
BETWEEN ... AND 会选取介于“两个值之间”的数据范围。这些值可以是数值、文本或者日期。
IN 和OR操作符允许我们在 WHERE 子句中规定“多个值”（多个值不一定要连续）。

这里id列是索引列，如果不是的话，三个查询都是全表扫描，性能差距应该不大。
SELECT * FROM tin where c1 >= 100 and c1 <= 104;（等价SELECT * FROM tin where c1 bewteen 100 and 104;）
SELECT * FROM tin where c1 in (100, 101, 102, 103, 104);
SELECT * FROM tin where c1 = 100 or c1 = 101 or c1 = 102 or c1 = 103 or c1 = 104;
对于语句1的WHERE条件十分简单，匹配上下界限即可，即对于每返回的一行数据需要两次验证，时间复杂度为常量O（2）。
对于语句2和语句3，则需要对IN或OR中的每个条件进行验证，知道找到某一匹配项为止，时间复杂度为O（n）。 
但是MySQL对于IN做了相应的优化，即将IN中的常量全部存储在一个有序数组里面，故匹配的时候是二分查找， 时间复杂度为O（lgn）。
在忽略I/O的情况下，仅仅从CPU的耗时来看，语句1应该是最少的，其次是IN，最差的就是OR
----------------------------------------------------------------------------

数据库范式是什么？（设计关系型数据库表的规范，越高的范式在“上一范式基础上”规范更多，相应数据库冗余（指数据重复）越小。
六种范式通常满足第三范式即可，下面通过订单--产品--厂商理解）
第一范式（1NF）：列不可分，
要求数据库表的每一列都是不可分割的原子数据项。
例如：联系方式：座机，手机分开

第二范式（2NF）：消除部份依赖（完全依赖），
在1NF基础上，确保数据库表中的每一列都和主键相关，而不能只与主键的某一部分相关（主要针对联合主键而言）。
例如：（订单ID，产品ID，产品价格）一个订单可以包含多个产品，但产品价格和订单ID无关，需要放另外一张表。

第三范式（3NF）：消除传递依赖
在2NF基础上，任何非主属性不依赖于其它非主属性
确保数据表中的每一列数据都和主键“直接”相关，而不能“间接”相关。
例如：产品ID，厂商ID，厂商地址。但是“厂商地址”直接依赖的是“厂商ID”，而不是主键“产品ID”


sql查询语句书写顺序、执行顺序
当它发现第一个词是SELECT关键字的时候，它会跳到FROM关键字，然后通过FROM关键字找到表名并把表装入内存，并形成一张虚表。
接着是找WHERE关键字，如果找不到则返回到SELECT找字段解析，如果找到WHERE，则分析WHERE后面返回值，真或假，
来确定接下来执不执行SELECT。如果为真那么把这条记录装到一个虚表当中，指针再指向下一条记录。
如果为假那么指针直接指向下一条记录，而不进行其它操作。一直检索完整个表，并把 检索出来的虚拟表返回给用户。
书写顺序：select （distinct）、from、join on、where、group by、having、order by（默认asc，desc）、limit
执行顺序：from、join on、where、group by、having、select （distinct）、order by、limit


内外连接（为什么性能更好？）
默认的交叉连接cross join，加条件只能用where,不能用on，
直接生成全表的笛卡儿积，左表的行数乘以右表的行数，在表很大时非常慢
select * from a,b 
	where a.id=b.id
--等价于
select * from a cross join b
	where a.id=b.id
而内外连接，在on的条件上筛选，避免生成全表的笛卡儿积
https://blog.csdn.net/qq_17707713/article/details/90047732
inner join、left join、right join、full outer join结合where条件实现AB表的七种并集交集组合。


给一个abc三个字段的索引，where a=0 order by c能用到索引吗？
用到索引a，并且单独对c排序
where a=0 and b大于0 order by c能用到索引吗？
可能用到索引a或者索引ab，b大于0的查询范围可能很大，索引也需要IO，当使用索引性能不如全表扫描，不会使用索引ab。并且单独对c排序

给一个数据库表，ID、score两个字段分别代表学生ID和成绩，写SQL语句求ID=？的学生排第几名？
？？？select count(score) from id_score where score>(select score from id_score where id = #{id})
假设是InnoDB，给上述SQL语句加索引怎么加？
score字段加上普通索引
--------------------------------------------------------------------------------
锁
InnoDB提供一致性非锁定读、行级锁，行级锁没有额外开销，同时保证并发性和一致性。
lock对象是事务，锁定的是表、页、行（三者依次包含关系），一般在事务commit或者rollback后释放（不同隔离级别释放时间不同），有死锁检测机制


为什么innodb死锁，myisam不会死锁？
上文讲过，MyISAM表锁是deadlock free的，这是因为“MyISAM总是一次获得所需的全部锁”，要么全部满足，要么等待，因此不会出现死锁。
但在InnoDB中，除单个SQL组成的事务外，锁是逐步获得的，这就决定了在InnoDB中发生死锁是可能的。
innodb死锁处理？
发生死锁后，InnoDB一般都能自动检测到，通过wait-for-graph“检测是否有回路”，若存在则有死锁，
“并使undo量最小的事务释放锁并回退”（剥夺），另一个事务获得锁，继续完成事务。
但在涉及外部锁，或涉及表锁的情况下，InnoDB并不能完全自动检测到死锁，这需要通过“设置锁等待超时参数 innodb_lock_wait_timeout”来解决。
需要说明的是，这个参数并不是只用来解决死锁问题，在并发访问比较高的情况下，如果大量事务因无法立即获得所需的锁而挂起，
会占用大量计算机资源，造成严重性能问题，甚至拖跨数据库。我们通过设置合适的锁等待超时阈值，可以避免这种情况发生。


InnoDB为了让表锁和行锁同时存在，而使用了意向锁（Intenyion lock）：
意向锁是“表级别”的锁，意向锁意味着事务想在“更细的粒度上（行）”加锁，意向锁不会阻塞全表扫以外的任何请求。
！两种表级锁：
意向共享锁（IS lock）：事务想要获得一张表某几行的共享锁
意向排他锁（IX lock）：事务想要获得一张表某几行的排他锁
！两种行级锁：
共享锁（S lock）：允许事务读一行数据；
排他锁（X lock）：允许事务删除或者更新一行数据

考虑这个例子：
事务A锁住了表中的一行，让这一行只能读，不能写。
之后，事务B申请整个表的写锁。
如果事务B申请成功，那么理论上它就能修改表中的任意一行，这与A持有的行锁是冲突的。
数据库需要避免这种冲突，就是说要让B的申请被阻塞，直到A释放了行锁。
数据库要怎么判断这个冲突呢？
step1：判断表是否已被其他事务用表锁锁表
step2：判断表中的每一行是否已被行锁锁住。
注意step2，这样的判断方法效率实在不高，因为需要遍历整个表。（！为什么使用意向锁？为什么意向锁是表级锁？）
于是就有了意向锁，直接判断一次就知道表中是否有数据行被锁定了。
在意向锁存在的情况下，事务A必须先申请表的意向共享锁，成功后再申请一行的行锁。
在意向锁存在的情况下，上面的判断可以改成
step1：不变
step2：发现表上有意向共享锁，说明表中有些行被共享行锁锁住了，因此，事务B申请表的写锁会被阻塞。
注意：事务A申请一行的行锁的时候，数据库会自动先开始申请表的意向锁，不需要我们程序员使用代码来申请。


Innodb3种锁算法：
1.record lock:单个行记录上的锁
总是锁住索引记录，如果没有设置索引，使用隐式的主键

2.gap lock:间隙锁，锁定一个范围，但不包含记录本身
阻止多个事务把记录插入到同一个范围内（间隙内）或者删除间隙内记录，为了解决幻读
对于键值在条件范围内但不存在的记录，叫做“间隙(GAP)”，InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁(gap lock)。

innodb 自动给符合条件的数据记录的索引项使用间隙锁的条件：
（1）必须在RR（可重复读）级别下
（2）检索条件必须有索引，且没有唯一属性（没有索引的话，mysql会全表扫描，那样会锁定整张表所有的记录，包括不存在的记录，此时其他事务不能修改不能删除不能添加）
（3）当我们用“范围条件”而不是相等条件检索数据
（4）（显示）“并请求共享或排他锁时”

最近的左右两个值作为边界，形成一个区间，“只要这些区间对应的两个记录中间可以插入记录，就认为区间对应的记录之间有间隙”
很多人会问，那记录（id=6，number=5）与（id=8，number=5）之间有间隙吗？
答案是有的，（id=6，number=5）与（id=8，number=5）之间可以插入记录（id=7，number=5）

“间隙锁的目的是为了防止幻读”，其主要通过两个方面实现这个目的：
（1）防止间隙内有新数据被插入或者删除
（2）防止已存在的数据，更新成间隙内的数据（例如防止numer=3的记录通过update变成number=5）

3.next-key lock:1+2，锁定一个范围，并且包含记录本身
mysql默认使用，当查询的索引没有唯一属性，使用next-key lock，即锁住索引本身（行锁针对查询行），同时锁住范围（间隙锁针对区间）
锁降级：
当查询的索引有唯一属性，next-key lock降级为record lock，即锁住索引本身（行锁），而不是范围。

MyISAM在执行查询语句（SELECT）前，会自动给涉及的所有“表”加读锁，
在执行更新操作（UPDATE、DELETE、INSERT等）前，会自动给涉及的“表”加写锁，这个过程并不需要用户干预。
MyISAM存储引擎的读锁和写锁是互斥的，读写操作是串行的。

意向锁是InnoDB自动加的，不需用户干预。
对于UPDATE、DELETE和INSERT语句，InnoDB会自动给“涉及数据集（无索引可能全表扫描，表锁）”加排他锁（X)；
对于普通SELECT语句，InnoDB不会加任何锁；
一致性的“锁定读”，某些情况需要“显示”对“读”操作加锁（必须写在事务里，事务提交锁释放），支持两种一致性的“锁定读”：共享锁或排他锁
begin trasaction
select ... for update“行记录”加X锁
select ... lock in share mode“行记录”加S锁


一致性“非”锁定读（consistent nonblocking read），通过MVCC多版本并发控制实现，多个版本是通过undo日志实现的。
当读取的行正在被写（update、delete）时，不会等待行锁的释放，
直接读取之前版本的数据（之前版本的数据不会修改，也不会有锁），并且不会被之后的修改或者未提交事务所影响。。
在读已提交、可重复读下，innodb均使用的一致性非锁定读。“读已提交总是读最新的版本，可重复读总是读取事务开始时的版本。”
读已提交违反了I隔离性。


MVCC多“版本（事务ID）”并发控制（乐观锁思想）
REPEATABLE READ采用的就是乐观锁，而乐观锁的实现采用的就是MVCC。
两个不同的事务相互之间不能影响，而且还能支持并发，这点悲观锁是达不到的。
InnoDB的MVCC,是通过在“每行记录”后面保存两个隐藏的列来实现的,分别保存了这个行的创建版本号和删除版本号，使用事务ID，
每开始一个新的事务，事务ID就会自动递增。
！Insert操作：
创建版本为当前事务ID，删除版本未定义
！Select操作：
InnoDB会根据以下两个条件检查每行记录:
1InnoDB只会查找版本早于当前事务版本的数据行(也就是,行的系统版本号小于或等于事务的系统版本号)，
“确保事务读取的行，要么是在事务开始前已经存在的，要么是事务自身插入或者修改过的.”
2行的删除版本要么未定义,要么大于当前事务版本号(“确保事务读取到的行，在事务开始之前未被删除”)， 
只有条件1、2同时满足的记录，才能返回作为查询结果.
！Delete操作：
删除版本为当前事务ID
！Update操作（实际上是新插入了一行记录）：
原记录的删除版本为当前事务ID，新增一条更新后的记录的创建版本为当前事务ID


--------------------------------------------------------------------------------
事务，ACID，mysql默认的隔离级别是可重复读，完全满足ACID
--------------A原子性---------------
事务是一个原子操作单元，其对数据的修改，要么全都执行，要么全都不执行。
--------------C一致性---------------
在事务开始和完成时，“数据”都必须保持一致状态，以保持数据的完整性。
一致性关注数据的可见性，中间状态的数据对外部不可见，只有最初状态和最终状态的数据对外可见。
举个粒子，张三给李四转账100元。事务要做的是从张三账户上减掉100元，李四账户上加上100元。
一致性的含义是其他事务要么看到张三还没有给李四转账的状态，要么张三已经成功转账给李四的状态，
而对于张三少了100元，李四还没加上100元这个中间状态是不可见的。
--------------I隔离性---------------
数据库系统提供一定的隔离机制，保证事务相互分离，该事务提交之前对其他事务不可见，同时保证一定并发度。
--------
id value
1   1
2   1
4   1
--------
事务A多次读id=2行，同时事务B多次修改id=2行，1-2-3-4-5
1. 脏读 事务A读取了2-3-4
脏读发生在一个事务A读取了被另一个事务B修改，但是“还未提交的数据”。
假如B回退，则事务A读取的是无效的数据。这跟不可重复读类似，但是第二个事务不需要执行提交。
2.不可重复读 事务A第一次读取了1，第二次读取了5
第一个事务读取数据，第二个事务修改并提交，第一个事务再次读取数据，两次读取不一样
！！！有两个策略可以防止这个问题的发生：
(1) 推迟事务B的执行，直至事务A提交或者回退。这种策略在“使用锁”时应用。
(2) mysql采用一致性“非”锁定读，在“多版本并发控制”中，事务B可以被先提交。而事务A，继续执行在旧版本的数据上。
当事务A终于尝试提交时，数据库会检验它的结果是否和事务A、事务B顺序执行时一样。如果是，则事务A提交成功。如果不是，事务A会被回退。
3.幻读 事务A多次读id<=4行，同时事务B插入id=3 value=1行。事务A第一次读取了3行，第二次读取了4行
重点在于新增或者删除 (数据条数变化)。同样的条件, 第1次和第2次读出来的记录数不一样
！！！解决办法：
在InnoDB下，Repeatable read隔离级别配合GAP间隙锁来避免幻读！
（检索范围、显示加行锁（共享锁，排他锁）后、满足隔离级别、索引条件自动添加、触发）


 	               脏读 	  不可重复读 	 幻读  
Serializable 	    不会 	    不会       	 不会  
REPEATABLE READ 	不会 	    不会       	 会    
READ COMMITTED  	不会 	    会         	 会    
Read Uncommitted	会   	    会         	 会
--------------D持久性---------------
事务完成之后，它对于数据的修改是永久性的，即使出现宕机也能够保持。
Innodb通过重做日志保证。
提供的高可靠性，而不是高可用性，例如外部磁盘损坏，并不保证持久性。


MySQL默认操作模式就是autocommit自动提交模式，每个查询都被当做一个单独的事务自动执行，在执行完语句之后自动commit。
1可以设置autocommit=0，即改成手动commit。
2可以显式地开始一个事务，mysql自动设置autocommit=0，并且ROLLBACK或COMMIT结束一个事务后，又自动设置autocommit=1。


下面是事务控制语句：

　　1、START TRANSACTION | BEGIN |BEGIN WORK :显示的开启一个事务

　　2、COMMIT ：提交事务。COMMIT之后事务对数据库的修改是持久性的

　　3、ROLLBACK：回滚事务。将未提交的事务进行回滚。

　　4、SAVEPOINT identifier ：给事务设置保存点

　　5、RELEASE SAVEPOINT identifier: 删除保存点

　　6、ROLLBACK TO [SAVEPOINT] identifier :回滚至保存点，保存点之前的操作还会生效。

　　7、SET TRANSACTION:设置数据库隔离级别。

注意rollback并不会使表空间缩小

带有保存点（savepoint）的扁平事务
某些事务可能在执行过程中出现的错误并不会导致所有的操作都无效，放弃整个事务不合乎要求，开销也太大。
保存点（Savepoint)用来通知系统应该记住事务当前的状态，以便当之后发生错误时，事务能回到保存点当时的状态。
对于默认的扁平事务来说，其隐式地设置了一个保存点，然而在整个事务中，只有这一个保存点，因此，回滚只能回滚到事务开始时的状态。


隔离性：表锁、行锁、乐观锁mvcc实现
原子性、持久性：通过redo log重做日志实现，恢复提交事务修改的操作。物理日志。
一致性：通过undo log撤销日志实现，恢复到某个特定版本。逻辑日志。
undo log除了用来回滚，另一个用处是mvcc。
！！！最重要的是，undo log的产生也会伴随redo log的产生，因为undo log也需要持久性的保护。

事务提交后不能马上删除undo log，因为可能还有其他事务需要通过undo log恢复到之前版本，
故放入一个链表，由purge线程判断删除。

分布式事务：如持卡人从交通银行转1000到中国银行
隔离级别必须设置serializable串行，当前的java的JTA（java transaction API）可以很好的支持mysql的分布式事务
采用两段式提交（two phase commit）：
第一个阶段，所有参与全局事务的节点都开始“准备”，告诉事务管理器它们准备好提交了。
第二个阶段，事务管理器告诉资源管理器执行rollback或者commit，“如果任何一个节点显示不能commit”，那么所有的节点就得全部rollback。

不好的事务习惯：
1.存储过程，循环中commit或者隐式提交
发生错误时，停留在未知位置；性能不如用一个事务包裹循环
2隐式提交
--------------------------------------------------------------------------------
备份：
热备：数据库运行过程中备份，对运行中的数据库无影响，
温备：数据库运行过程中备份，对运行中的数据库有影响，但仅支持读请求，不允许写请求，可能加一个全局锁保证一致性
冷备：数据库停止情况下，复制相关物理文件


一般情况下, 我们需要备份的数据分为以下几种：
数据
二进制日志, InnoDB事务日志
代码(存储过程、存储函数、触发器、事件调度器)
服务器配置文件


mysql复制原理：异步实时地将二进制日志重做传送并应用到从数据库


针对不同的场景下, 我们应该制定不同的备份策略对数据库进行备份, 一般情况下, 备份策略一般为以下4种：
1.直接cp,tar复制数据库文件
2.mysqldump+复制BIN LOGS
3.lvm2快照+复制BIN LOGS
4.xtrabackup
以上的几种解决方案分别针对于不同的场景
如果数据量较小, 可以使用第一种方式, 直接复制数据库文件
如果数据量还行, 可以使用第二种方式, 先使用mysqldump对数据库进行完全备份, 然后定期备份BINARY LOG达到增量备份的效果
如果数据量一般, 而又不过分影响业务运行, 可以使用第三种方式, 使用lvm2的快照对数据文件进行备份, 而后定期备份BINARY LOG达到增量备份的效果
如果数据量很大, 而又不过分影响业务运行, 可以使用第四种方式, 使用xtrabackup进行完全备份后, 定期使用xtrabackup进行增量备份或差异备份

